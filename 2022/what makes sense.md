# Naming things
The big idea behind GoF's Design Patterns was that by giving recurring patterns a name, you make it easier for developers to share ideas and visualize each other's solutions. For some patterns, this really took off, like if you mention "singleton" everyone knows what you're talking about. However, if you mention "builder", it will likely conjure different mental images and lead to confusion. If you say "flyweight", people will likely just stare at you. The easier the pattern is conceptually 
(or the more common), the more likely people will recognize the name.

Over the years, other areas adopted new patterns with their own names, to the point where now there are dozens of well-established patterns. I qualify them as well-established if there's a general consensus around one name and you can find a wikipedia article. I would argue, even when GoF was published, that there were already too many for anyone to remember. Even if someone forced themselves to remember all the pattern names, it's likely without hands-on experience that the name wouldn't conjure the right mental image or they wouldn't really know when/how to apply it.

Myself, I read GoF, and afterward convinced myself that I knew what all the patterns meant. Years later, when I finally needed a pattern, visitor, I realized I sorely misunderstood it. I wouldn't have even considered it if someone hadn't specifically asked for it on one of my open source projects. Now, I have used it to solve real problems on several projects over the years. I recently re-read some of GoF and realized my memory had faded or was corrupted with time. In other cases, I never reached out for a pattern even when it would have been appropriate because it wasn't ingrained. Anyone who thinks they have all the GoF patterns down are lying to themselves (*probably*).

Now, we have architectural patterns, concurrency patterns, integration patterns, and so on. There's so many patterns, and they concern so many different areas, that no developer has reason to remember them all. A decent book on any of these areas would probably cost $30-$50, too, so a dedicated programmer just spent (what?) $150 or more and months of reading? But, let's be honest, no one writes books like that anymore.

## Should we bother?
I read an article recently where a large company (Uber) claimed that they didn't employee any sort of architect. They simply evaluated the problems and designed solutions. The team was large enough that in reality they probably had dozens of sub-projects that just needed integrated later. I can believe an architect could be left out of the equation. If each sub-project was small enough and self-contained enough... maybe things could work out. I have my doubts about this story, though.

Any experienced software developer is going to be able to make decent decisions, I'd hope. There's a tendency, early on in our careers, to make things complicated by default, without realizing it. We learn, hopefully, how to avoid complexity as we go. I am not sure how true this is - I could see a developer getting their start at a company where the software is in a bad state. Without an opportunity to see how to do things well, or an opportunity to try to do it better, would a developer come to learn a better way? Maybe if they felt particularly tortured by their situation, they would seek out advice and stumble across how things *could* be - I know I did.

I've also know too many senior developers who have been trying to build the same software for a large part of their careers. What they did at the start of their careers was atypical, but they saw it working. Without seeking out guidance on a better approach, they just keep rebuilding the same architecture. Each time, they think, "Hey, I can do it even better this time!" and they end up just making it more complex. Without even considering the requirements, they pick what they're familiar with and write thousands of lines of code. So maybe "architect" anymore just means a developer who's too concerned with "perfecting the formula".

Languages tend to lead to complexities in their own right. Many languages today benefit from their core developer base working in limited IDEs - basically beefed-up text editors. These languages often use simple files and directory structures to organize code, rather than use project files. They use plain text files, too, rather than GUI drag'n'drop tools written out to proprietary formats or unwieldy XML files. They don't require a server running in the background. I started working in Java a few years ago and it still feels very enterprise-y. People are still talking about EJBs, JavaEE, and monolithic frameworks. Still, I never felt comfortable developing in anything other than Visual Studio for C# development, and I bet that's still the case.

With simple programming tools, simple libraries, and general guidelines (a.k.a., best practices), can we build good software? Just avoiding "formulas" could prove to be beneficial all its own.

## A bad example
I worked at a small company a few years ago. They had a lot of recent college grads trying to rebuild their core product. That might be good enough explanation for why the project failed, right there. When I came onto the project, it was already two years old, still under heavy development, and still not deployed to production in any fashion.

There were a couple of really bad decisions. To implement messaging (a.k.a., event signaling), they used some obscure mechanism in Python. The idea was that mechanism could be swapped out at runtime, based on a configuration setting, to allow the same code to run on one machine or distributed across many machines. When distributed, a different event handler would just forward the message to RabbitMQ instead of a local object. In theory, this sounds cool. In reality, this mechanism in Python was not well-understood or documented. It also didn't force the developers to create well-defined interfaces, which is problematic when going across boundaries. Martin Fowler says that remote interfaces should be coarse-grained, mostly for performance reasons, but I would say also to force developers to recognize where the boundaries *are*.

My personal belief is that a lot of the issues they were running into was related to runaway software complexity. The original developer(s) were gone. They didn't have a clear separation between layers. They had a CI/CD set up that required developers to have VirtualBox running all the time, and the actual CI/CD process was not at all understood by the development team. They were obsessed with 100% code coverage, but their tests were a mess.

Someone early on saying "No, this is too complicated" would have solved a lot of problems. Being made up of a lot of recent graduates, the culture was everyone trying to prove they were smart, so humbly admitting something was "too complicated" was not going to happen. CI/CD only being required on check-in would have solved a lot of problems (or waiting 5 years for Docker to become popular 😅). Detecting that OPS was going to become a huge bottleneck because VPN security could only be configured by one person would have helped, too.

Overall, most of the problems could have been resolved by empowering developers to improve their own situations. A part of me doesn't want to discuss OPS or CI/CD, because I want to focus on functional requirements; however, developers not considering how things get onto an actual machine is an area that often causes an otherwise healthy project to fail or get delayed - it happened to me.

So, in my opinion, you can't just let a group of inexperienced developers run wild on a project an expect success. You also can't empower people stuck in their ways, taking the power away from the rest of the team. Most of all, you need people who are able to say "I don't know" and do a little research, and then share their findings with the team.

## KISS, YAGNI, SOLID and long hours
Over the years, I have lead several small-to-medium sized projects. They ranged from 1 to 7 team members, and from thousands to hundreds of thousands of lines of code. They were all successful, and on target. I had a manager ask me one time why I thought my projects were successful, and at the time I really didn't know. I had this notion of a team member who, basically, had the whole thing in their heads (a.k.a., me). On a small project, that is certainly possible. On a large project, that's almost impossible. In some cases, the reason the project was successful was that *I* was doing an unfair amount of the coding, while also keeping the team busy, dealing with customers, preparing for demos, and so on. Now, I think it's more about having a high-level notion about what's coming down the road and staying mostly focused on delivering in short turnaround cycles. That means handing off working code every two weeks or so.

As long as you have the big picture (50,000ft view, as my first lead called it), you can identify what you know and what you don't know. You can make decisions that avoid lots of rework later. You'll still need to reorganize now and then, sure. That's a good thing.

Between time constraints and avoiding the unknown, you are forced to adopt YAGNI (You Aren't Going to Need It). Less code - less to maintain. For testing, this often led to writing fewer tests, focusing on common use cases, or adopting an "Only write tests to reproduce bugs" approach. When I didn't follow this principle, approximately 50% of time spent on projects was spent writing tests. Tests were very hard to maintain and made reorganization difficult and unrewarding. While I see the long-term benefit to writing tests, I think there needs to be a lot more discussion on how to prioritize when and what tests are written. Is happy path good enough? Do you add missing tests when you are extending functionality later down the road? I also found high-level tests, performing a full user story basically, to require minimal maintenance and to highlight most introduced bugs (a.k.a., "oops, I broke something"). These can be ran directly though a REST API, or the classes directly under that, depending on what's easiest and how well structured the code is. Using true domain models (logic that is completely independent of database and UI concerns) can reduce how often changes in dependencies (via DI) result in updating tests, as well.

I think keeping things simple plays a big part in a project's success. I don't like magic. I don't like heavy reflection. I don't like convoluted inheritance hierarchies. If I can't just start in a "main" method and step my way through the code, I am not very much interested in working on the project. I like the idea that a junior developer can come onto a project and not need to know the names of 50 different patterns to be productive, or spend 5 weeks reading about all the unfamiliar technologies being employed.

The SOLID principles are a strange lot. Some things, like single responsibility, are universal truths that most people are epically poor at recognizing. Liskov substitution principle is a trivia question during a bad interview, though. The potency of OOP is wearing off, so OOP-specific concepts have less weight now. This same thing applies to the Open-Close principle, where high-order functions seem more the norm these days. While still a OOP concept, I find interface segregation far more useful, as it is often used along with dependency injection to solve complicated problems, but mostly interface segregation acts like a poor-man's traits. Of all of the SOLID principles, though, single responsibility is the one people should focus on.

### Type more, automate less
It might sound contrary, but I see a lot of developers wasting time trying to automate things that involve runtime costs, rather than just typing something out by hand. For example, people will write micro-ORMs to generate a handful of SELECT statements across their application. When the ORM can't spit out the correct SQL, for whatever reason, they reach out for "injecting" raw bits of SQL into the overall query or waste more time making the ORM more flexible. I see people using runtime reflection to find a bunch of classes or methods, when they could have just listed them in code (or simply introduced an interface and checked for it via `instanceof` 😏). While these types of projects can be very rewarding, they are rarely justified, cost-wise.

This sort of stuff often leads to "magic" no one else on the team understands. When a bug comes up, rather than simply stepping through the code, developers find themselves jumping around trying to figure where to stick their breakpoint. I've also worked on systems with home-grown ORMs where you ultimately "just need to write a raw SQL" and you can't because the database connection is hidden behind 30 layers of abstraction. In these cases, the cost grows even higher.

## Example: Separate models for JSON serialization
On one of the projects I work on now, the original developer did not use JSON serialization in a *typical* manner. Correctly, he did not want to ship the database models unaltered to the front-end. Instead, he manually built a JSON object by specifying each field to serialize. This approach makes it very difficult to send computed values. If multiple rows are returned, it requires caching information ahead-of-time that would otherwise result in a separate database hit for each row. Creating nested objects is basically impossible. On the way back to the backend, there's completely separate code to map back to the database. Since it involves reflection, there's code to make sure only "bindable" values can be bound, for security reasons. Neither side of this serialization scheme is understood by the team.

You do not want to serialize your database/domain models - trust me. Sending database models is rarely even feasible because database models are often self-referential. For example, a `User` will often have an accessor for the "created by" `User`. When a JSON serializer sees this, it will try to serialize the "created by" user as a nested JSON object. That user, too, will have a "created by" user, leading to an infinite loop. I have seen good developers waste days trying to understand what is happening in this situation. From their perspective, they just see a `StackOverflowException`, and have no idea why. They are often hard to diagnose.

The solution is simple: create a separate model that gets serialized to JSON. Such a model is made up of primitives, collections, and other JSON-friendly models, free of self-referencing classes. This model also closely corresponds to what the UI/client is expecting, so they do not need to perform any computations on their end.

I think most developers know you should not send database models to your UI, simply because it's a violation of layering. I don't think most developers really appreciate what that really means, though. I recently saw an enum, which was part of the data layer, that had CSS classes associated with it. It's unnerving to be at the bottom of the call stack and see developers hard-coding CSS there. Does using separate layers imply the use of separate models? If so, why do I see so many examples in the wild where people invented their own serialization mechanism or wasted time tracking down mysteries? Is this simple solution not obvious? Obviously not.

I think the reason why such an obvious solution is ignored is that, at first, it seems like a lot of typing. In most of my applications, my UI/API models look almost identical to my database models. Not only do I have to write an almost identical class, I have to write a mapper class that copies values from one model to the other. It feels like a waste of time. It's so common that tools like AutoMapper exist in .NET. 

> AutoMapper is the very definition of the sort of magic I hate: the kind that trades runtime and readability to save a developer from some typing. And, since it relies on reflection, renaming a property on either side of the mapping breaks the mapping. It's not even particularly easy to specify a mapping manually, and dealing with mappings from one-to-many or many-to-one requires a lot of brain power. I've seen cases where the mapping configuration was as complicated as writing the mapper by hand would have been. Just write the mapper, please!

### It's easy to get yourself in trouble
I know on my first project I considered making POJOs/POCOs for my data layer. I definitely wanted type-safe accessors for my data. At that time, .NET was heavily into using `DataTable` to map data. It felt stupid to create a completely independent class, map each `DataRow` to said class, update it, and then map the changes back to the `DataRow` (which involved a look up). Instead, it seemed more performant to simply wrap each `DataRow` with said class and have the getters/setters/properties read from and update the underlying `DataRow` directly.

The challenge ended up being new records. In that case, I had to ask the `DataTable` for a new `DataRow`. Now, imagine you're at the top of your application, just beginning to process a request from the UI. Down at the very bottom of your application is a `DataTable` that you'll eventually use to persist changes to the database. You somehow need to access a *typed* `DataTable` in the persistence layer that should be all but invisible to te UI layer, just to create a new record. That then gets passed down until you can finally add it to the `DataTable` after all the values are set. It's doable, but since creating a typed `DataRow` involved instantiating a typed `DataTable`, you tend to want to reuse the same `DataTable`, which would mean bringing it up to the top of the application again, violating layering. So you either initialize a typed `DataTable`, asking it for a row, and then just throw it away, or you violate layer. Maybe cache a typed `DataTable` that's just for creating new rows? Seems complicated.

Using POJOs/POCOs, there's no issue creating new objects, as this is just using the default constructor. The UI layer could have completely initialize the new object before passing it down to the data layer. There, a mapper would have just created a new `DataRow` and copied the values into it. A fair question is whether the application was complex enough to warrant this much layering. I would say not... but most applications aren't that straight-forward. This is also an era where web pages were rendered on the backend, so in-memory objects, not serialized JSON, could be used to build pages. You could get away with less layering in general without running into issues. This was a bit of a double-edged sword, as that sort of decision making could cripple a project later on when complexity finally did come along. In fact, I encountered many monstrosities from that era due to excessive coupling and there being no home for domain logic.

## Conclusion
So, I don't think a project can be successful without some oversight. The hope is your senior developers have seen things like separate JSON models along the way, and know to use them. You hope someone on the team knows how to work with a relational database. You hope someone on the team understands CI/CD and the mechanics of production deployments. You also hope the people on that team have an appreciation for simple, readable, discoverable code. You don't need an architect, but you do need experience.