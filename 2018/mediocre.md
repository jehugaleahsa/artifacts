# Can you afford mediocrity?
It's a well-studied and accepted fact that some developers are dramatically more productive than others. What makes that interesting is that the more productive developers also tend to produce fewer defects. So it's not just an up-front boost in productivity, it's a long-term reduction in both development and maintenance cost. You can read more about this interesting topic by googling "10x Developer".

Another interesting aspect about 10x developers is that years of experience have nothing to do with it. In fact, many studies have shown a developer's skill level is pretty much set in stone early on. Some people claim programmers' brains are just wired up to be good at programming. I find the notion that coding ability is pre-determined particularly interesting because I know throughout my career I picked up on some fundamental development skills, like OOP and functonal programming, that literally altered the way I write code. That said, before learning those skills, I could achieve the same results in less elegant ways. In a sense, learning new techniques only helped me write more solid code more quickly. I'm regularly reassured I'm on the right track when new language features come along making what I'm already doing easier.

There seem to be an endless number of articles describing how to attract and retain highly-skilled developers, even 10x developers. Obviously the techniques that are outlined do not really work, or we'd stop seeing new articles every other week. :-) As explained in Peopleware, highly skilled developers naturally gravitate toward one another and the companies that allow them to achieve their best work. A lot of companies don't accept this plain fact because it makes the situation seem hopeless. It's not, however, because companies have the ability to adopt practices that great developers value. If you get lucky enough to snag a 10x developer, you have all the ingredients to build a super team. Because a junior is just as likely to be an inexperienced 10x developer, this is not as impossible as it might seem at first.

What no one seems to talk about is whether companies actually need top-notch developers to begin with. The studies give the impression that the difference in cost is astronomical. That's actually only the case when you're comparing below-average developers to 10x developers. Most developers are neither bad at their jobs nor particularly great -- they're mediocre. In which case, they may be 3-to-5 times more productive than a truly awful developer but only half as productive as a 10x programmer. Since the likelihood you'll ever encounter a 10x developer is so low, a 5x developer is nothing to scoff at. In fact, you should be extremely greatful.

Let's talk about urgency. If you are in an extremely competitive market where short release cycles is your only advantage, then a 10x programmer can be the difference between raking in the dough and going out of business. For most businesses, this is not the case. Most businesses already have an established customer base who are invested in their products. They are willing to wait a few months (maybe even years) for a new feature. In my experience, most development efforts are upgrading older technologies, automating business processes or adding a sprinkle of new features. That means if there was a delay, the most it would result in is a slightly irritated customer or a slight loss in hypothetical profit margins (probably less than the cost of new development). 

## Retention
Most managers wouldn't recognize a 10x developer if they had one. Most managers don't have the authority to provide real incentive to keep them around anyway. Even in 2018, what developers spend most of their time doing is a complete mystery to the average person, even most project managers. If you are a project manager who coded in the 1990's or early 2000's, you probably wouldn't recognize most of what developers are doing today. I've been programming for 10 years, now, and a lot has changed. Fundamentally, we are just writing code but there's a whole lot more to it.

This morning, I spent several hours constructing an enormous SQL (800+ lines) for reading data from one system into my own. I didn't have access to the other system directly, so I had to use SQL Server's data import wizard to make copies of the other system's data. Is it okay that I only made a copy? In this case, yes. The other system had a very denormalized structure (repeated groups of data) and my system is more normalized. I basically converted one table into four. After writing the query, I sent out an email to let people know it was available for *whenever it was time for me to start developing*... yeah. Even after I finish development, it is just to generate some sample data so another team can decide if they agree with my design.

So writing code is only a small part of my job some days, yet managers tend to focus on the coding activity, which is why estimates always seem so off. Your average developer isn't very good at incorporating non-development tasks into their estimates. It's often the case that you only realize you have some missing prerequisites when you go to type and you hit the wall. Here, experience can actually help. This type of experience explains why estimates tend to get better as agile projects progress. I would argue one thing that makes an above average developer is they *can* see beyond the coding activity. That involves being able to visualize yourself coding and fast-forwarding through the boring parts. In fact, self-observation is *the* one core attribute of a great developer.

So we buffer our estimates. There's less visibility that way. Since things are done "on time", there's no effort spent analyzing where the time went, so there's little experience gained to make better estimates in the future. Consultant companies often believe they can *eventually* boost productivity and achieve a competitive advantage, cost-wise, by specializing in a particular industry -- creating "domain experts", so called. Comically, companies will maintain this delusion decade after decade. In the modern work world, there's too much turnover -- your company hits a reset button every 10 years or so. Nor is there that much repetition even within the same industry. Otherwise, you'd be a product company by now. Maybe buffering is okay since there's little opportunity for gaining experience. Ultimately, better estimates fall on the developer.

## Metadata
I've noticed some problems are beyond your average developer. Most developers can read a requirements document and produce a conforming blob of code that executes step 1, then 2, etc. It works, yay! Some developers have the rare ability to look beyond 1, 2, 3 and see a higher-level set of concepts. Perhaps 1, 2, and 3 share a lot in common and some extra metadata is all that's needed to stick them in a collection and loop over them.

The ability to recognize higher-level patterns by injecting metadata is very rare indeed. It's the difference between writing a custom CRM for your company and building an industrial-stength, general-purpose CRM used by thousands of companies. Far rarer, though, is the ability to codify these sorts of abstractions. Rarer still is the ability to organize this type of code into something another person can understand (establishing metaphores).

Your company probably doesn't need an insanely nifty way of implementing your requirements. In the short-term, simple code costs less. Adding new features to simple solutions just means piling on more code. Veterans of a code base often discover, long after a project goes live, ways to make the code easier to maintain and enhance. DRY, as a practice, can achieve many of the desired abstractions without needing to be a genius. Again, DRY requires self-observation and discipline.

While refactoring bits here and there can ease new development, veteran developers know deep down that what they *really* need to do is a fundamental rewrite. A good developer would see these patterns while still in development, before a total rewrite is required. A fantastic developer would attain up-front simplicity while building in the flexibility to do more complex things later. That like what a software architect does, without the over-engineering.

I've encountered many large systems in my day that could have benefitted from a dash of metadata. Whenever onboarding new clients involves writing more code, your system is probably missing fundamental customization points. Supporting customization without modifying the core code requires metadata. Building a general-purpose tool requires metadata. Allowing admins to change the way the system behaves at run-time requires metadata. Most great 3rd-party libraries only work if given metadata.

Yet, your company probably isn't building industrial-strength software...

## Cheaper long-term
A great developer can build software faster and higher-quality than a whole team of mediocre developers. Even an expensive developer is cheaper than a whole team. The challenge is great developers want to surround themselves with smart people (*all developers need companionship*). There's no point in hiring a single 10x developer -- they'll just turn around and leave. If you get lucky and hire a great junior, don't rely on them staying cheap for long. In other words, you don't want to become too dependent on a single, great developer. Always budget for a team (at least two people), even if that means they're all mediocre. Don't think you can pair mediocre and great developers for long either.

Here's the disconnect: you can't tell the quality of a developer by their price tag and interviews aren't all-revealing either, but this also means you only pay slightly more for a great developer. Good luck convincing a company that 10-25% more for a developer will mean 200-1,000% more productivity. If you are trying to build a team around a great developer, put him or her in charge of hiring the team -- make it clear there should be no reservations and give him the last say. It may take *dozens* of interviews, so be ready for it to take months.

Think long-term, too. If a project is under constant development, having the same great developers working on the code will ultimately be cheaper. But, if the goal is to get a solution to a stable state with minimal maintenance and the occasional new feature, paying less for developers over the long run will save money. When stable, a single low-cost developer can take over and the rest of the team moves on. Honestly, though, low-quality developers will never manager to get software into a stable state. You'll be lucky if mediocre developers can do it cost-effectively.

What about starting with a high-quality developer to get a solution in place faster and then switching to a cheaper team afterwards? This doesn't work very well, as it turns out. Great developers tend to adopt newer technology early along with guidelines, princples, best practices, etc. This can be particularly challenging for low-paid developers, who get paid for results, not quality. This is why so many developers continued using jQuery inside of their AngularJS code. It took several years before AngularJS became pervasive enough that low-paid developers got on-board. Ultimately, the quality will suffer and don't even think about handing that contaminated code back to the original developer.